{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('Crash_Reporting_-_Drivers_Data.csv', low_memory=False)\n",
    "cleaned_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle Make cleaning\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['TOYT','TOYO','TOYTA', 'TOY'], 'TOYOTA')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('HOND', 'HONDA')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['CHEVY', 'CHEV','CHEYV'], 'CHEVROLET')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('NISS', 'NISSAN')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['HYUN','HYUNDIA','HYUNUNDAI','HYUNUDAI','HYUUNDAI'], 'HYUNDAI')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['MERCEDES-BE','MERCEDESS','MERCEDES ZBENZ','MERCEDES ENZ','MERCEDEZ BENS','MERCEDS','MERDECES','MERCRDEZ BENZ','MERCEZ','MERCENDES','MERCEES','MERCEEDEZ','MERCEES','MERZ','MERCEDES', 'MERC','MERCEDES BENZ','MERCEDEZ','MEZ','MERZEDEZ','MERZEDES BENZ','MERZ 4S','MERX','MERS-BENZ','MERECEDEZ','MERDEDES','MERDECES-BENZ','MERCRY','MERDECES-BENZ'], 'MERCEDES-BENZ')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['VOLK','VOLKS','VW'], 'VOLKSWAGEN')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('ACUR', 'ACURA')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('SUBA', 'SUBARU')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('DODG', 'DODGE')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['THOM', 'THOMAS','THMS'], 'THOMAS BUILT')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('MAZD', 'MAZDA')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['LEXS','LEXU','LESUX','LEXUUS','LEXUSZ','LEXUSS','LEXI'], 'LEXUS')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('FRHT', 'FREIGHTLINER')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('VOLV', 'VOLVO')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('INFI', 'INFINITI')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['GILL','GILG','GILLLIG BU','GILLS','GILLMAN'],'GILLIG')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('CHRY', 'CHRYSLER')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('BUIC', 'BUICK')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['CADI','CADDILAC','CADILACC','CADILLA'], 'CADILLAC')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace(['INFINITY','INFINITIG'], 'INFINITI')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('LINC', 'LINCOLN')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('INTL', 'INTERNATIONAL')\n",
    "cleaned_data['Vehicle Make'] = cleaned_data['Vehicle Make'].replace('NISSIAN', 'NISSAN')\n",
    "\n",
    "#Substance cleaning\n",
    "cleaned_data['Driver Substance Abuse'] = cleaned_data['Driver Substance Abuse'].str.upper()\n",
    "cleaned_data['Driver Substance Abuse'] = cleaned_data['Driver Substance Abuse'].replace('UNKNOWN, UNKNOWN', 'UNKNOWN')\n",
    "cleaned_data['Driver Substance Abuse'] = cleaned_data['Driver Substance Abuse'].replace('SUSPECT OF ALCOHOL USE, UNKNOWN', 'SUSPECT OF ALCOHOL USE, NOT SUSPECT OF DRUG USE')\n",
    "\n",
    "#Traffic controls\n",
    "cleaned_data['Traffic Control'] = cleaned_data['Traffic Control'].replace('TRAFFIC CONTROL SIGNAL', 'TRAFFIC SIGNAL')\n",
    "cleaned_data['Traffic Control'] = cleaned_data['Traffic Control'].replace('FLASHING TRAFFIC CONTROL SIGNAL','FLASHING TRAFFIC SIGNAL')\n",
    "cleaned_data['Traffic Control'] = cleaned_data['Traffic Control'].fillna('UNKNOWN')\n",
    "\n",
    "# Weather\n",
    "cleaned_data['Weather'] = cleaned_data['Weather'].str.upper()\n",
    "cleaned_data['Weather'] = cleaned_data['Weather'].fillna('UNKNOWN')\n",
    "\n",
    "\n",
    "#Surface condition\n",
    "cleaned_data['Surface Condition'] = cleaned_data['Surface Condition'].str.upper()\n",
    "cleaned_data['Surface Condition'] = cleaned_data['Surface Condition'].replace(['WATER(STANDING/MOVING)','WATER (STANDING, MOVING)'],'WET')\n",
    "cleaned_data['Surface Condition'] = cleaned_data['Surface Condition'].replace('ICE/FROST','ICE')\n",
    "\n",
    "\n",
    "#Light\n",
    "cleaned_data['Light'] = cleaned_data['Light'].str.upper()\n",
    "cleaned_data['Light'] = cleaned_data['Light'].replace('DARK - LIGHTED','DARK LIGHTS')\n",
    "cleaned_data['Light'] = cleaned_data['Light'].replace('DARK - NOT LIGHTED','DARK NO LIGHTS')\n",
    "cleaned_data['Light'] = cleaned_data['Light'].replace(['DARK -- UNKNOWN LIGHTING','DARK - UNKNOWN LIGHTING'],'DARK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing Differences in Upper and Lower case strings\n",
    "cleaned_data['Vehicle Damage Extent'] = cleaned_data['Vehicle Damage Extent'].str.upper()\n",
    "cleaned_data['Injury Severity'] = cleaned_data['Injury Severity'].str.upper()\n",
    "cleaned_data['Traffic Control'] = cleaned_data['Traffic Control'].str.upper()\n",
    "cleaned_data['Surface Condition'] = cleaned_data['Surface Condition'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the subset for label propegation\n",
    "columns = ['Speed Limit','Injury Severity','Collision Type','Weather','Circumstance','Surface Condition','Light','Traffic Control','Vehicle First Impact Location','Vehicle Damage Extent']\n",
    "subset = cleaned_data[columns]\n",
    "subset = subset.dropna()\n",
    "#adding a label for the final label propegation data\n",
    "large_data = cleaned_data[cleaned_data['Circumstance'].isna()]\n",
    "large_data = large_data[columns]\n",
    "large_data = large_data.drop('Circumstance', axis=1)\n",
    "large_data['Surface Condition'].fillna('UNKNOWN')\n",
    "large_data['Predicted_Label'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Circumstance\n",
       "RAIN, SNOW, WET                                   11590\n",
       "N/A, WET                                           8480\n",
       "N/A, RAIN, SNOW                                    1043\n",
       "SLEET, HAIL, FREEZ. RAIN, WET                      1001\n",
       "N/A, VISION OBSTRUCTION (INCL. BLINDED BY SUN)      962\n",
       "Failed to Yield Right-of-Way                        949\n",
       "ICY OR SNOW-COVERED, RAIN, SNOW                     887\n",
       "ANIMAL, N/A                                         676\n",
       "BACKUP DUE TO REGULAR CONGESTION, N/A               648\n",
       "ICY OR SNOW-COVERED, N/A                            621\n",
       "N/A, ROAD UNDER CONSTRUCTION/MAINTENANCE            456\n",
       "Followed Too Closely                                426\n",
       "Other Improper Action                               416\n",
       "DEBRIS OR OBSTRUCTION, N/A                          292\n",
       "ICY OR SNOW-COVERED, SLEET, HAIL, FREEZ. RAIN       253\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset['Circumstance'].value_counts().head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6810/1322676498.py:9: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  subset.loc[subset['Circumstance'].str.contains('|'.join(non_collision_keywords), case=False, na=False), 'Collision'] = 0\n"
     ]
    }
   ],
   "source": [
    "# Define keywords for collisions (1) and non-collisions (0)\n",
    "collision_keywords = ['Other Improper Action','Followed Too Closely','Followed Too Closely ','Improper Passing','Too Fast For Conditions','Ran Red Light','Failed to Keep in Proper Lane','Failed to Yield Right-of-Way','Operated Motor Vehicle in Inattentive, Careless, Negligent, or Erratic Manner', 'Improper Turn']\n",
    "non_collision_keywords = ['ANIMAL','VISION OBSTRUCTION (INCL. BLINDED BY SUN)','ICY OR SNOW-COVERED, RAIN, SNOW', 'BACKUP DUE TO REGULAR CONGESTION', 'DEBRIS OR OBSTRUCTION']\n",
    "\n",
    "# Assign labels:\n",
    "\n",
    "subset['Collision'] = -1  # Default all to -1\n",
    "subset.loc[subset['Circumstance'].str.contains('|'.join(collision_keywords), case=False, na=False), 'Collision'] = 1\n",
    "subset.loc[subset['Circumstance'].str.contains('|'.join(non_collision_keywords), case=False, na=False), 'Collision'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collision\n",
       "-1    26694\n",
       " 1     3950\n",
       " 0     3050\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset['Collision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.preprocessing import OneHotEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jianderi/anaconda3/lib/python3.11/site-packages/sklearn/semi_supervised/_label_propagation.py:231: RuntimeWarning: invalid value encountered in divide\n",
      "  probabilities /= normalizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Labels in Subset:\n",
      "Predicted_Label\n",
      "0    27153\n",
      "1     6541\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Predicted Labels in Large Dataset:\n",
      "Predicted_Label\n",
      "0    90345\n",
      "1    64515\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define categorical features\n",
    "categorical_cols = ['Surface Condition', 'Weather', 'Traffic Control', 'Speed Limit']\n",
    "\n",
    "# One-Hot Encode categorical features for the subset\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=True)\n",
    "X_categorical_subset = encoder.fit_transform(subset[categorical_cols])\n",
    "\n",
    "# Prepare labels for Label Propagation\n",
    "y_subset = subset['Collision']\n",
    "\n",
    "# Train Label Propagation Model\n",
    "label_prop_model = LabelPropagation(kernel='knn')\n",
    "label_prop_model.fit(X_categorical_subset, y_subset)\n",
    "\n",
    "# Transform the large dataset\n",
    "X_categorical_large = encoder.transform(large_data[categorical_cols])\n",
    "\n",
    "# Predict labels\n",
    "subset['Predicted_Label'] = label_prop_model.predict(X_categorical_subset)\n",
    "large_data['Predicted_Label'] = label_prop_model.predict(X_categorical_large)\n",
    "\n",
    "# Output results\n",
    "print(\"Predicted Labels in Subset:\")\n",
    "print(subset['Predicted_Label'].value_counts())\n",
    "\n",
    "print(\"\\nPredicted Labels in Large Dataset:\")\n",
    "print(large_data['Predicted_Label'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted_Label\n",
      "0    117498\n",
      "1     71056\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure both datasets have the same columns\n",
    "subset['Predicted_Label'] = subset['Predicted_Label'].astype(int)  # Ensure dtype consistency\n",
    "large_data['Predicted_Label'] = large_data['Predicted_Label'].astype(int)  \n",
    "\n",
    "# Combine into a master dataset\n",
    "master_data = pd.concat([subset, large_data], ignore_index=True)\n",
    "master_data.drop(['Collision','Circumstance'], axis=1, inplace=True)\n",
    "# Check the shape and column consistency\n",
    "print(master_data['Predicted_Label'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data.to_csv('Labeled_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
